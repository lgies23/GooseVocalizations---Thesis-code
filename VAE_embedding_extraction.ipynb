{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114616970>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls_df = pd.read_csv('features_and_spectrograms.csv')\n",
    "calls_df['log_padded_spectrogram'] = calls_df['log_padded_spectrogram'].apply(lambda x: np.array(json.loads(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7300 entries, 0 to 7299\n",
      "Data columns (total 33 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   track_ID                7300 non-null   object \n",
      " 1   clip_ID                 7300 non-null   object \n",
      " 2   goose_ID                7300 non-null   object \n",
      " 3   call_type               7300 non-null   object \n",
      " 4   waveform                7300 non-null   object \n",
      " 5   sr                      7300 non-null   int64  \n",
      " 6   filepath                7300 non-null   object \n",
      " 7   lfccs                   7300 non-null   object \n",
      " 8   peak                    7300 non-null   float64\n",
      " 9   duration                7300 non-null   float64\n",
      " 10  normalized_log_length   7300 non-null   float64\n",
      " 11  log_target_duration     7300 non-null   float64\n",
      " 12  log_padded_spectrogram  7300 non-null   object \n",
      " 13  log_padded_lfccs        7300 non-null   object \n",
      " 14  f0mean                  6484 non-null   float64\n",
      " 15  f0range                 6484 non-null   float64\n",
      " 16  f0min                   6484 non-null   float64\n",
      " 17  f0max                   6484 non-null   float64\n",
      " 18  f0std_dev               6484 non-null   float64\n",
      " 19  mean_slope              6484 non-null   float64\n",
      " 20  f0_q1                   6484 non-null   float64\n",
      " 21  f0_q2                   6484 non-null   float64\n",
      " 22  f0_q3                   6484 non-null   float64\n",
      " 23  hnr                     7300 non-null   float64\n",
      " 24  centr_s                 7300 non-null   float64\n",
      " 25  skew_s                  7300 non-null   float64\n",
      " 26  kurt_s                  7300 non-null   float64\n",
      " 27  std_s                   7300 non-null   float64\n",
      " 28  centr_t                 7300 non-null   float64\n",
      " 29  skew_t                  7300 non-null   float64\n",
      " 30  kurt_t                  7300 non-null   float64\n",
      " 31  std_t                   7300 non-null   float64\n",
      " 32  avg_f_form1             6484 non-null   float64\n",
      "dtypes: float64(23), int64(1), object(9)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "calls_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6484 entries, 0 to 7299\n",
      "Data columns (total 5 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   track_ID                6484 non-null   object\n",
      " 1   clip_ID                 6484 non-null   object\n",
      " 2   goose_ID                6484 non-null   object\n",
      " 3   call_type               6484 non-null   object\n",
      " 4   log_padded_spectrogram  6484 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 303.9+ KB\n"
     ]
    }
   ],
   "source": [
    "print(len(calls_df[calls_df.isna().any(axis=1)]))\n",
    "calls_df = calls_df.dropna()\n",
    "calls_df = calls_df[[\"track_ID\", \"clip_ID\", \"goose_ID\", \"call_type\", \"log_padded_spectrogram\"]]\n",
    "calls_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(calls_df[\"call_type\"])\n",
    "calls_df[\"encoded_call_type\"] = le.transform(calls_df[\"call_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 256 # very unbalanced dataset so choosing a bigger batch size - TODO balance\n",
    "epochs = 60\n",
    "retrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 170)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calls_df.iloc[0][\"log_padded_spectrogram\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS(Dataset):\n",
    "    # Custom data loader \n",
    "    def __init__(self, calls_df):\n",
    "        self.df = calls_df\n",
    "                \n",
    "    # of items in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.df)    \n",
    "    \n",
    "    # Get item through index\n",
    "    def __getitem__(self, idx):\n",
    "        spectrogram = torch.from_numpy(self.df.iloc[idx][\"log_padded_spectrogram\"]).to(torch.float32).to(device)\n",
    "        spectrogram = spectrogram[np.newaxis, ...]\n",
    "        return spectrogram, self.df.iloc[idx][\"encoded_call_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SoundDS(calls_df)\n",
    "\n",
    "# Random split for train:val - 80:20\n",
    "num_items = len(ds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(ds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([256, 1, 44, 170])\n",
      "Labels batch shape, type: torch.Size([256])\n",
      "44 170\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dl))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape, type: {train_labels.size()}\")\n",
    "\n",
    "input_size=train_features.size()[1:]\n",
    "print(input_size[1], input_size[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            # input shape (n, 1, 44, 170)\n",
    "            torch.nn.Conv2d(1, 8, (4,4), (2,2), (1,2)), # output shape (n, 8, 22, 86)\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(8, 4, (4,4), (2,2), (1,1)), # output shape (n, 4, 11, 43)\n",
    "            torch.nn.BatchNorm2d(4),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Flatten(), # output shape (n, 1280)\n",
    "            torch.nn.Linear(1892, 512),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(512, 128),\n",
    "        )\n",
    "\n",
    "        # latent mean and variance \n",
    "        self.mean_layer = torch.nn.Linear(128, 2)\n",
    "        self.logvar_layer = torch.nn.Linear(128, 2)\n",
    "        \n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 512),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(512, 1892),\n",
    "            torch.nn.Unflatten(1, (4, 11, 43)),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.BatchNorm2d(4),\n",
    "            torch.nn.ConvTranspose2d(4, 8, (4,4), (2,2), (1,1)),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.ConvTranspose2d(8, 1, (4,4), (2,2), (1,2)),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)      \n",
    "        z = mean + var*epsilon\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encode(x)\n",
    "        z = self.reparameterization(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalAutoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 2))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Conv2d(8, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=1892, out_features=512, bias=True)\n",
      "    (8): LeakyReLU(negative_slope=0.01)\n",
      "    (9): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=512, out_features=1892, bias=True)\n",
      "    (3): Unflatten(dim=1, unflattened_size=(4, 11, 43))\n",
      "    (4): LeakyReLU(negative_slope=0.01)\n",
      "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ConvTranspose2d(4, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 2))\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvolutionalAutoencoder().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    # written by https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60:\n",
      " training loss=0.1291 \n",
      " validation loss=0.4395\n",
      "Epoch 2/60:\n",
      " training loss=0.1252 \n",
      " validation loss=0.4384\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#-------- train --------\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 12\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Compute the loss and perform backpropagation\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/goose_vocalizations/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/goose_vocalizations/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/goose_vocalizations/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/goose_vocalizations/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/goose_vocalizations/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mSoundDS.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 12\u001b[0m     spectrogram \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_padded_spectrogram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     spectrogram \u001b[38;5;241m=\u001b[39m spectrogram[np\u001b[38;5;241m.\u001b[39mnewaxis, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spectrogram, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoded_call_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "early_stopper = EarlyStopper(patience=5, min_delta=1e-3)\n",
    "\n",
    "if retrain:\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_val_loss = 0.0\n",
    "        #-------- train --------\n",
    "        model.train()\n",
    "        for batch_idx, (X, y) in enumerate(train_dl):\n",
    "            # Forward pass\n",
    "            encoded, decoded = model(X)\n",
    "            # Compute the loss and perform backpropagation\n",
    "            loss = loss_fn(decoded, X)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the running loss\n",
    "            total_train_loss += loss.item() * X.size(0)\n",
    "            if early_stopper.early_stop(total_train_loss):             \n",
    "                break\n",
    "\n",
    "        # Calculate epoch loss\n",
    "        epoch_train_loss = total_train_loss / len(train_dl.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "\n",
    "        #------- validate -------\n",
    "        model.eval()\n",
    "        for batch_idx, (X, y) in enumerate(val_dl):\n",
    "            # Forward pass\n",
    "            encoded, decoded = model(X)\n",
    "\n",
    "            # Compute the loss and perform backpropagation\n",
    "            val_loss = loss_fn(decoded, X)\n",
    "\n",
    "            # Update the running loss\n",
    "            total_val_loss += val_loss.item() * X.size(0)\n",
    "\n",
    "        # Calculate epoch loss\n",
    "        epoch_val_loss = total_val_loss / len(val_dl.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        print(\n",
    "            \"Epoch {}/{}:\\n training loss={:.4f}\".format(epoch + 1, epochs, epoch_train_loss),\n",
    "            \"\\n validation loss={:.4f}\".format(epoch_val_loss)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goose_vocalizations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
